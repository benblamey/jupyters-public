{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11110"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter \n",
    "import itertools\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://ben-spark-master:7077\") \\\n",
    "        .config('spark.executor.memory', '2g') \\\n",
    "        .config('spark.driver.maxResultSize', 0) \\\n",
    "        .appName(\"common_crawl\")\\\n",
    "        .getOrCreate()\n",
    "#        .config('spark.executor.cores', 2)\\\n",
    "\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# (*/*) - out of memoryâ€º\n",
    "# ~6.4mins for 39496 files. (...00000/)  (takes 1 minute with 40 partitions)\n",
    "# ~5 secs for 10 files (...00000/0*) \n",
    "# ~20 secs for 11110 files (...00000/1*) \n",
    "\n",
    "rdd = spark_context.wholeTextFiles('/mnt/nfs/ben-spark-master/teaching/crawl/CC-MAIN-2018-03/splits/CC-MAIN-20180317035630-20180317055630-00000.warc.wet/1*', minPartitions=40)\\\n",
    ".cache() # Keep this RDD in memory!\n",
    "# Get 40 partitions here.\n",
    "rdd.count()\n",
    "# Only one job (previous .cache() did not trigger a job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_context.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 245),\n",
       " ('of', 152),\n",
       " ('the', 149),\n",
       " ('to', 91),\n",
       " ('Diploma', 78),\n",
       " ('University', 74),\n",
       " ('Intermediate', 72),\n",
       " ('Research', 67),\n",
       " ('', 60),\n",
       " ('Design', 56),\n",
       " ('in', 53),\n",
       " ('a', 48),\n",
       " ('for', 47),\n",
       " ('at', 45),\n",
       " ('&', 45),\n",
       " ('|', 42),\n",
       " ('Engineering', 38),\n",
       " ('About', 36),\n",
       " ('by', 34),\n",
       " ('Browse', 34),\n",
       " ('Cambridge', 33),\n",
       " ('Department', 30),\n",
       " ('The', 30),\n",
       " ('PhD', 26),\n",
       " ('on', 25),\n",
       " ('students', 25),\n",
       " ('-', 24),\n",
       " ('Contact', 24),\n",
       " ('with', 24),\n",
       " ('research', 23),\n",
       " ('Publications', 23),\n",
       " ('your', 23),\n",
       " ('International', 22),\n",
       " ('Media', 21),\n",
       " ('News', 21),\n",
       " ('Bristol', 21),\n",
       " ('Courses', 20),\n",
       " ('Search', 19),\n",
       " ('Business', 19),\n",
       " ('How', 19),\n",
       " ('more', 19),\n",
       " ('Undergraduate', 19),\n",
       " ('Us', 19),\n",
       " ('Study', 19),\n",
       " ('Student', 19),\n",
       " ('Current', 19),\n",
       " ('you', 18),\n",
       " ('2', 18),\n",
       " ('Alumni', 18),\n",
       " ('pp.', 18),\n",
       " ('School', 18),\n",
       " ('study', 17),\n",
       " ('Warwick', 17),\n",
       " ('our', 17),\n",
       " ('Information', 17),\n",
       " ('Find', 16),\n",
       " ('Events', 16),\n",
       " ('Services', 16),\n",
       " ('2017', 16),\n",
       " ('be', 15),\n",
       " ('Boero,', 15),\n",
       " ('is', 15),\n",
       " ('this', 14),\n",
       " ('2013', 14),\n",
       " ('Overview', 14),\n",
       " ('WRAP', 14),\n",
       " ('us', 14),\n",
       " ('5', 14),\n",
       " ('10', 13),\n",
       " ('P', 13),\n",
       " ('Skip', 13),\n",
       " ('English', 13),\n",
       " ('4', 13),\n",
       " ('Education', 13),\n",
       " ('11', 13),\n",
       " ('Birmingham', 13),\n",
       " ('9', 13),\n",
       " ('1', 12),\n",
       " ('Law', 12),\n",
       " ('it', 12),\n",
       " ('education', 12),\n",
       " ('Social', 12),\n",
       " ('Laboratory', 12),\n",
       " ('Frank', 12),\n",
       " ('AA', 12),\n",
       " ('6', 12),\n",
       " ('Home', 12),\n",
       " ('Sciences', 12),\n",
       " ('8', 12),\n",
       " ('Urbanism', 12),\n",
       " ('Postgraduate', 12),\n",
       " ('Year', 12),\n",
       " ('Art', 11),\n",
       " ('section...', 11),\n",
       " ('Read', 11),\n",
       " ('Williams,', 11),\n",
       " ('Built', 11),\n",
       " ('will', 11),\n",
       " ('Graduate', 11),\n",
       " ('service', 11)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example #1 - Filter by TLD and compute most common words ##\n",
    "\n",
    "# Try .ac.uk, .ru, .se, .com\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.ac.uk', re.IGNORECASE)\n",
    "\n",
    "rdd\\\n",
    ".filter(lambda doc: bool(p.search(doc[1])))\\\n",
    ".map(lambda filename_content: filename_content[1].partition('\\r\\n\\r\\n')[2])\\\n",
    ".flatMap(lambda t: t.split(' '))\\\n",
    ".flatMap(lambda w: w.split('\\n'))\\\n",
    ".map(lambda w: w.strip()).cache()\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(100, key=lambda x: -x[1])\n",
    "\n",
    "# .cache().take(5)\n",
    "\n",
    "# See: http://spark.apache.org/docs/2.3.0/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Example #2 - Group by TLD and compute most common words for each ##\n",
    "\n",
    "ex = \"WARC-Type: conversion\\\n",
    "WARC-Target-URI: http://news.bbc.co.uk/2/hi/africa/3414345.stm\\\n",
    "WARC-Date: 2014-08-02T09:52:13Z\"\n",
    "\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.([a-zA-Z]{2,3})/', re.IGNORECASE)\n",
    "# print(p.search(ex).group(1))\n",
    "# uk\n",
    "\n",
    "def get_tld(content):\n",
    "    match = p.search(content)\n",
    "    if match is not None:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "words_by_tld_rdd = rdd\\\n",
    ".map(lambda filename_content: filename_content[1])\\\n",
    ".map(lambda content: (get_tld(content), content.partition('\\r\\n\\r\\n')[2]))\\\n",
    ".filter(lambda tld_content: tld_content[0] is not None)\\\n",
    ".flatMapValues(lambda words: words.split(' '))\\\n",
    ".flatMapValues(lambda words: words.split('\\n'))\\\n",
    ".mapValues(lambda word: word.strip())\n",
    "#.take(10)\n",
    "\n",
    "print(words_by_tld_rdd.take(10))\n",
    "\n",
    "tlds = words_by_tld_rdd.countByKey()\n",
    "#print(tlds)\n",
    "\n",
    "tlds = OrderedDict(sorted(tlds.items(), key = itemgetter(1), reverse = True))\n",
    "print(tlds)  \n",
    "\n",
    "top_tlds = dict(itertools.islice(tlds.items(), 10))\n",
    "\n",
    "print(top_tlds)\n",
    "\n",
    "print(\"Results:\")\n",
    "\n",
    "for tld in top_tlds:\n",
    "    print(tld)\n",
    "    top_words_for_tld = words_by_tld_rdd\\\n",
    "        .filter(lambda tld_word: tld_word[0] == tld)\\\n",
    "        .values()\\\n",
    "        .map(lambda w: (w,1))\\\n",
    "        .reduceByKey(add)\\\n",
    "        .takeOrdered(20, key=lambda x: -x[1])\n",
    "    print(top_words_for_tld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_content = rdd.take(1)[0][1]\n",
    "#print(file_content.partition('\\r\\n\\r\\n')[2])\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "ex = \"WARC-Type: conversion\\\n",
    "WARC-Target-URI: http://news.bbc.co.uk/2/hi/africa/3414345.stm\\\n",
    "WARC-Date: 2014-08-02T09:52:13Z\"\n",
    "\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.(([a-zA-Z]{2,3}}\\.)?[a-zA-Z]{2,3}})/', re.IGNORECASE)\n",
    "\n",
    "print(p.search(ex))\n",
    "\n",
    "#print(bool(p.search('\\nWARC-Target-URI:\\n')))\n",
    "\n",
    "#rdd\\\n",
    "#.filter(lambda doc: bool(p.search(doc[1])))\\\n",
    "#.map(lambda filename_content: filename_content[1].partition('\\r\\n\\r\\n')[2])\\\n",
    "#.flatMap(lambda t: t.split(' '))\\\n",
    "#.flatMap(lambda w: w.split('\\n'))\\\n",
    "#.map(lambda w: w.strip())\\\n",
    "#.map(lambda w: (w,1))\\\n",
    "#.reduceByKey(add)\\\n",
    "#.takeOrdered(100, key=lambda x: -x[1])\n",
    "#.take(100)\n",
    "#.take(10)\n",
    "#.flatMap(lambda text: text.split(' ')).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
