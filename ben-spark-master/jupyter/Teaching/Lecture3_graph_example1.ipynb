{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://ben-spark-master:7077\") \\\n",
    "        .appName(\"page_rank\")\\\n",
    "        .config('spark.executor.cores', 2)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_urls\n",
      "['1', '6', '2', '4', '3', '5']\n",
      "\n",
      "initial ranks:\n",
      "[('1', 1.0), ('6', 1.0), ('2', 1.0), ('4', 1.0), ('3', 1.0), ('5', 1.0)]\n",
      "new ranks:\n",
      "[('4', 0.15), ('5', 3.125), ('6', 1.0), ('2', 0.15), ('1', 0.575), ('3', 0.15)]\n",
      "new ranks:\n",
      "[('1', 1.478125), ('2', 0.15), ('3', 0.15), ('5', 0.9575), ('4', 0.15), ('6', 1.5418749999999999)]\n",
      "new ranks:\n",
      "[('1', 0.5569375), ('2', 0.15), ('5', 1.7251562499999995), ('4', 0.15), ('6', 0.6206875), ('3', 0.15)]\n",
      "new ranks:\n",
      "[('3', 0.15), ('4', 0.15), ('2', 0.15), ('5', 0.9421468749999999), ('6', 0.9469414062499998), ('1', 0.8831914062499998)]\n",
      "new ranks:\n",
      "[('2', 0.15), ('1', 0.550412421875), ('5', 1.2194626953124996), ('3', 0.15), ('6', 0.6141624218749999), ('4', 0.15)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 has rank: 0.15.\n",
      "1 has rank: 0.550412421875.\n",
      "5 has rank: 1.2194626953124996.\n",
      "3 has rank: 0.15.\n",
      "6 has rank: 0.6141624218749999.\n",
      "4 has rank: 0.15.\n"
     ]
    }
   ],
   "source": [
    "# Based on: https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "NUMBER_ITERATIONS = 5\n",
    "\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "lines = spark_session.sparkContext.parallelize(\n",
    "                       ['1 5',\n",
    "                        '2 5',\n",
    "                        '3 5',\n",
    "                        '4 5',\n",
    "                        '5 1',\n",
    "                        '5 6',\n",
    "                        '2 6'])\n",
    "\n",
    "#lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors.\n",
    "parsed_lines = lines.map(lambda urls: parseNeighbors(urls))\n",
    "links = parsed_lines.distinct().groupByKey().cache()\n",
    "# (source, [target,target,target])\n",
    "#print(links.take(10))\n",
    "#break\n",
    "\n",
    "\n",
    "#print(links.flatMapValues(lambda targets: targets).flatMap(lambda x: x).take(10))\n",
    "#break\n",
    "\n",
    "all_urls = parsed_lines.flatMap(lambda source_target: [source_target[0], source_target[1]]).distinct()\n",
    "print(\"\\nall_urls\")\n",
    "print(all_urls.take(10))\n",
    "\n",
    "\n",
    "#print(\"links (with neighbours):\")\n",
    "#print(links.take(10))\n",
    "\n",
    "# Loads all URLs with other URL(s) linked to from input file and initialize ranks of them to one.\n",
    "ranks = all_urls.map(lambda source_targets: (source_targets[0], 1.0))\n",
    "# (source, rank_integer)\n",
    "print(\"\\ninitial ranks:\")\n",
    "print(ranks.take(10))\n",
    "\n",
    "\n",
    "for iteration in range(NUMBER_ITERATIONS):\n",
    "    # join(): match keys, combine values into 2-tuple: (k, (v1, v2))\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "        # For each URL, compute its contrib to other URLs (based on the rank of each source)\n",
    "        lambda source__targets_rank: computeContribs(source__targets_rank[1][0], source__targets_rank[1][1]))\n",
    "    # (target, rank_contribution_to_target)\n",
    "    #print(contribs.take(10))\n",
    "\n",
    "    new_ranks_from_sources = contribs.reduceByKey(add)\n",
    "    #print(new_ranks_from_sources.take(10))\n",
    "    \n",
    "    # Re-calculates URL ranks based on linking source URL contributions.\n",
    "    ranks = ranks.leftOuterJoin(new_ranks_from_sources).mapValues(lambda oldrank_newrank: (oldrank_newrank[1] or 0) * 0.85 + 0.15)\n",
    "\n",
    "    print(\"\\nnew ranks:\")\n",
    "    print(ranks.take(10))\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "    \n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "     print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
